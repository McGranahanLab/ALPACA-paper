{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T15:01:07.948984Z",
     "start_time": "2024-04-21T15:01:07.930585Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly.io as pio   \n",
    "pio.kaleido.scope.mathjax = None\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import sigfig\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import wilcoxon\n",
    "import os,sys\n",
    "# set correct wd:\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith('ANALYSIS'):\n",
    "    os.chdir('./bin/ANALYSIS')\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "output_directory = '../../output'\n",
    "os.makedirs(f'{output_directory}/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison between HATCHet2 and ALPACA\n",
    "def get_hd_hatchet_alpaca_colours(palette):\n",
    "    import json\n",
    "    with open(palette, \"r\") as f:\n",
    "        palette = json.load(f)\n",
    "    catgorical_palette = palette[\"categorical\"]\n",
    "    colours = {}\n",
    "    colours[\"hd\"] = catgorical_palette[\"c12\"]\n",
    "    colours[\"hatchet\"] = catgorical_palette[\"c8\"]\n",
    "    colours[\"alpaca\"] = catgorical_palette[\"c2\"]\n",
    "    return colours\n",
    "\n",
    "\n",
    "def create_consensus_segments(df1, df2,position_columns_names=['#CHR','START','END']):\n",
    "    chr_colname = position_columns_names[0]\n",
    "    start_colname = position_columns_names[1]\n",
    "    end_colname = position_columns_names[2]\n",
    "    combined = pd.concat([\n",
    "        df1[[chr_colname, start_colname]].rename(columns={start_colname: \"position\"}),\n",
    "        df1[[chr_colname, end_colname]].rename(columns={end_colname: \"position\"}),\n",
    "        df2[[chr_colname, start_colname]].rename(columns={start_colname: \"position\"}),\n",
    "        df2[[chr_colname, end_colname]].rename(columns={end_colname: \"position\"}),\n",
    "    ])\n",
    "    \n",
    "    combined = combined.drop_duplicates().sort_values(by=[chr_colname, \"position\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Create consensus segments\n",
    "    consensus = combined.copy()\n",
    "    consensus[end_colname] = consensus.groupby(chr_colname)[\"position\"].shift(-1)\n",
    "    consensus = consensus.dropna().rename(columns={\"position\": start_colname})\n",
    "    consensus = consensus[[chr_colname, start_colname, end_colname]]\n",
    "    \n",
    "    def map_metadata(consensus, original,metadata_columns):\n",
    "        consensus = consensus.copy()\n",
    "        for col in metadata_columns:\n",
    "            consensus[col] = None\n",
    "        \n",
    "        # Map metadata by checking overlaps\n",
    "        for _, row in original.iterrows():\n",
    "            overlap = (consensus[chr_colname] == row[chr_colname]) & \\\n",
    "                      (consensus[start_colname] >= row[start_colname]) & \\\n",
    "                      (consensus[end_colname] <= row[end_colname])\n",
    "            for col in metadata_columns:\n",
    "                consensus.loc[overlap, col] = row[col]\n",
    "        return consensus\n",
    "\n",
    "    metadata_columns_df1 = [col for col in df1.columns if col not in position_columns_names]\n",
    "    metadata_columns_df2 = [col for col in df2.columns if col not in position_columns_names]\n",
    "    consensus_df1 = map_metadata(consensus, df1,metadata_columns_df1)\n",
    "    consensus_df2 = map_metadata(consensus, df2,metadata_columns_df2)\n",
    "    consensus_df1 = consensus_df1.dropna()\n",
    "    consensus_df2 = consensus_df2.dropna()\n",
    "    \n",
    "    \n",
    "    consensus_df1['segment'] = consensus_df1[position_columns_names].apply(lambda x: f'{x[chr_colname]}_{int(x[start_colname])}_{int(x[end_colname])}',axis=1)\n",
    "    consensus_df2['segment'] = consensus_df2[position_columns_names].apply(lambda x: f'{x[chr_colname]}_{int(x[start_colname])}_{int(x[end_colname])}',axis=1)\n",
    "    \n",
    "    consensus_df1['seg_len'] = consensus_df1[end_colname] - consensus_df1[start_colname]\n",
    "    consensus_df2['seg_len'] = consensus_df2[end_colname] - consensus_df2[start_colname]\n",
    "    \n",
    "    # remove zero-length segments:\n",
    "    consensus_df1 = consensus_df1[consensus_df1['seg_len'] > 0]\n",
    "    consensus_df2 = consensus_df2[consensus_df2['seg_len'] > 0]\n",
    "    \n",
    "    # keep only common segments:\n",
    "    common_segments = set(consensus_df1['segment']).intersection(set(consensus_df2['segment']))\n",
    "    consensus_df1 = consensus_df1[consensus_df1['segment'].isin(common_segments)]\n",
    "    consensus_df2 = consensus_df2[consensus_df2['segment'].isin(common_segments)]\n",
    "    assert consensus_df1.shape[0] == consensus_df2.shape[0]\n",
    "    return consensus_df1, consensus_df2\n",
    "\n",
    "\n",
    "def get_score_for_segment(segment_df_true,segment_df_model,clone_columns_true,clone_columns_model):\n",
    "    set_of_predicted_cns = set(sum([x.split('|') for x in segment_df_model[clone_columns_model].values[0]],[]))\n",
    "    set_of_true_cns = set(sum([x.split('|') for x in segment_df_true[clone_columns_true].values[0]],[]))\n",
    "    assert len(set_of_predicted_cns) > 0\n",
    "    assert len(set_of_true_cns) > 0\n",
    "    seg_len = segment_df_true['seg_len'].values[0]\n",
    "    recovered = len(set_of_predicted_cns.intersection(set_of_true_cns))\n",
    "    all_states = len(set_of_predicted_cns.union(set_of_true_cns))\n",
    "    predicted_denom = len(set_of_predicted_cns)\n",
    "    true_denom = len(set_of_true_cns)\n",
    "    precision = seg_len * (recovered / predicted_denom)\n",
    "    recall = seg_len * (recovered / true_denom)\n",
    "    accuracy = seg_len * (recovered / all_states)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "\n",
    "def get_metrics_per_case(model_consensus,TRUE_consensus,clone_columns_true,clone_columns_model):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    accuracy_list = []\n",
    "    for segment in model_consensus['segment']:\n",
    "        segment_df_model = model_consensus[model_consensus['segment']==segment]\n",
    "        segment_df_true = TRUE_consensus[TRUE_consensus['segment']==segment]\n",
    "        precision,recall,accuracy = get_score_for_segment(segment_df_true,segment_df_model,clone_columns_true,clone_columns_model)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        accuracy_list.append(accuracy)\n",
    "    total_len = sum(model_consensus['seg_len'])\n",
    "    precision = sum(precision_list)/total_len\n",
    "    recall = sum(recall_list)/total_len\n",
    "    accuracy = sum(accuracy_list)/total_len\n",
    "    return precision,recall,accuracy\n",
    "\n",
    "\n",
    "def load_H1_results(dataset_name,experiment,hatchet1_output_dir):\n",
    "    wgd_results_path = f'{hatchet1_output_dir}/WGD/{dataset_name}/{experiment}/hatchet/hatchet.seg.ucn.gz'\n",
    "    nowgd_results_path = f'{hatchet1_output_dir}/noWGD/{dataset_name}/{experiment}/hatchet/hatchet.seg.ucn.gz'\n",
    "    if os.path.exists(wgd_results_path):\n",
    "        H1_results = pd.read_csv(wgd_results_path, sep='\\t')\n",
    "    else:\n",
    "        H1_results = pd.read_csv(nowgd_results_path, sep='\\t')\n",
    "    H1_results['#CHR'] = H1_results['#CHR'].apply(lambda x: x.replace('chr',''))\n",
    "    H1_results['START'] = H1_results['START'].astype(int)\n",
    "    H1_results['END'] = H1_results['END'].astype(int)\n",
    "    H1_results.rename(columns={'cn_normal':'cn_clone100'},inplace=True)\n",
    "    position_columns_names=['#CHR','START','END']\n",
    "    H1_results_clones = [x for x in H1_results.columns if x.startswith('cn_')]\n",
    "    H1_results = H1_results[position_columns_names + H1_results_clones].drop_duplicates() \n",
    "    return H1_results\n",
    "\n",
    "\n",
    "def load_clone_HD_results(dataset_name,experiment,hatchet1_output_dir):\n",
    "    wgd_results_path = f'{hatchet1_output_dir}/WGD/{dataset_name}/{experiment}/clonehd/cloneHD.BEST.results.gz'\n",
    "    nowgd_results_path = f'{hatchet1_output_dir}/noWGD/{dataset_name}/{experiment}/clonehd/cloneHD.BEST.results.gz'\n",
    "    if os.path.exists(wgd_results_path):\n",
    "        clone_hd_results = pd.read_csv(wgd_results_path, sep='\\t')\n",
    "    else:\n",
    "        clone_hd_results = pd.read_csv(nowgd_results_path, sep='\\t')\n",
    "    # fix column names:\n",
    "    clone_hd_results.rename(columns={'END':'SAMPLE','SAMPLE':'END'},inplace=True)\n",
    "    assert clone_hd_results['END'].dtype.type == np.int64\n",
    "    assert clone_hd_results['SAMPLE'].dtype.type == np.object_\n",
    "    \n",
    "    clone_hd_results['#CHR'] = clone_hd_results['#CHR'].apply(lambda x: x.replace('chr',''))\n",
    "    clone_hd_results['START'] = clone_hd_results['START'].astype(int)\n",
    "    clone_hd_results['END'] = clone_hd_results['END'].astype(int)\n",
    "    clone_hd_results.rename(columns={'cn_normal':'cn_clone100'},inplace=True)\n",
    "    position_columns_names=['#CHR','START','END']\n",
    "    clone_hd_results_clones = [x for x in clone_hd_results.columns if x.startswith('cn_')]\n",
    "    clone_hd_results = clone_hd_results[position_columns_names + clone_hd_results_clones].drop_duplicates() \n",
    "    return clone_hd_results\n",
    "\n",
    "\n",
    "def load_alpaca_hatchet_format(alpaca_output_dir,tumour_id):\n",
    "    alpaca_results = pd.read_csv(f'{alpaca_output_dir}/patient_outputs/{tumour_id}/final_{tumour_id}.csv')[['clone','pred_CN_A','pred_CN_B','segment']]\n",
    "    normal_clones = ['diploid']\n",
    "    alpaca_results = alpaca_results[~alpaca_results.clone.isin(normal_clones)]\n",
    "    alpaca_results['cn'] = alpaca_results[['pred_CN_A','pred_CN_B']].apply(lambda x: f'{x[0]}|{x[1]}',axis=1)\n",
    "    alpaca_results.drop(['pred_CN_A','pred_CN_B'],axis=1,inplace=True)\n",
    "    # to wide format:\n",
    "    alpaca_results_wide = alpaca_results.pivot(index='segment',columns='clone',values='cn').reset_index()\n",
    "    alpaca_results_wide['#CHR'] = alpaca_results_wide['segment'].apply(lambda x: x.split('_')[0])\n",
    "    alpaca_results_wide['START'] = alpaca_results_wide['segment'].apply(lambda x: int(x.split('_')[1]))\n",
    "    alpaca_results_wide['END'] = alpaca_results_wide['segment'].apply(lambda x: int(x.split('_')[2]))\n",
    "    alpaca_results_wide.drop(['segment'],axis=1,inplace=True)\n",
    "    return alpaca_results_wide\n",
    "\n",
    "\n",
    "def load_true_copynumber_states(hatchet_sim_true_cns,h2_name_no_suffix):\n",
    "    true_copynumber_states_nowgd_path = f'{hatchet_sim_true_cns}/noWGD/{h2_name_no_suffix}/tumor/copynumbers.csv'\n",
    "    true_copynumber_states_wgd_path = f'{hatchet_sim_true_cns}/WGD/{h2_name_no_suffix}/tumor/copynumbers.csv'\n",
    "    true_copynumber_states = pd.read_table(true_copynumber_states_nowgd_path, sep='\\t') if os.path.exists(true_copynumber_states_nowgd_path) else pd.read_csv(true_copynumber_states_wgd_path, sep='\\t')\n",
    "    true_copynumber_states['#CHR'] = true_copynumber_states['#CHR'].apply(lambda x: x.replace('chr',''))\n",
    "    true_copynumber_states['START'] = true_copynumber_states['START'].astype(int)\n",
    "    true_copynumber_states['END'] = true_copynumber_states['END'].astype(int)\n",
    "    true_copynumber_states['clone100'] = '1|1'\n",
    "    return true_copynumber_states\n",
    "\n",
    "\n",
    "def get_model_metrics(model_results, true_copynumber_states):\n",
    "    MODEL_consensus,TRUE_consensus = create_consensus_segments(model_results, true_copynumber_states)\n",
    "    clone_columns_model = [x for x in MODEL_consensus.columns if 'clone' in x]\n",
    "    if len(clone_columns_model) == 0:\n",
    "        clone_columns_model = [x for x in MODEL_consensus.columns if 'cn_' in x]\n",
    "    clone_columns_true = [x for x in TRUE_consensus.columns if 'clone' in x]\n",
    "    precision,recall,accuracy = get_metrics_per_case(MODEL_consensus,TRUE_consensus,clone_columns_true,clone_columns_model,)\n",
    "    return {'precision':precision,'recall':recall,'accuracy':accuracy}\n",
    "\n",
    "\n",
    "def make_plot(results_H1_df=pd.Series(),results_ALPACA_df=pd.Series(),results_cloneHD_df=pd.Series(),colours={},metric='accuracy', font_size=45,w=600,h=800,fonts='Arial'):\n",
    "    y_axis_title = metric.capitalize()\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(title='', font=dict(family=fonts, size=font_size), width=w, height=h, showlegend=False, plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',)\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='black', zeroline=True, zerolinecolor='black', zerolinewidth=1, title=y_axis_title, title_font=dict(family=fonts, size=font_size*0.8),)\n",
    "    jitter_value = 0.5\n",
    "    point_size = font_size/10\n",
    "    line_width = 2\n",
    "    if len(colours) == 0:\n",
    "        hatchet_colour = 'blue'\n",
    "        alpaca_colour = 'orange'\n",
    "        clonehd_colour = 'green'\n",
    "    else:\n",
    "        alpaca_colour = colours['alpaca']\n",
    "        hatchet_colour = colours['hatchet']\n",
    "        clonehd_colour = colours['hd']\n",
    "    \n",
    "    if len(results_cloneHD_df)>0:\n",
    "        cloneHD_trace = go.Box(y=results_cloneHD_df[metric], orientation='v', boxpoints='all', jitter=jitter_value, pointpos=0, line=dict(color=clonehd_colour, width=line_width), marker=dict(size=point_size),name='cloneHD')\n",
    "        fig.add_trace(cloneHD_trace)\n",
    "        \n",
    "    if len(results_H1_df)>0:\n",
    "        hatchet1_trace = go.Box(y=results_H1_df[metric], orientation='v', boxpoints='all', jitter=jitter_value, pointpos=0, line=dict(color=hatchet_colour, width=line_width), marker=dict(size=point_size), name='HATCHet2')\n",
    "        fig.add_trace(hatchet1_trace)\n",
    "    \n",
    "    if len(results_ALPACA_df)>0:\n",
    "        alpaca_trace = go.Box(y=results_ALPACA_df[metric], orientation='v', boxpoints='all', jitter=jitter_value, pointpos=0, line=dict(color=alpaca_colour, width=line_width), marker=dict(size=point_size),name='ALPACA')\n",
    "        fig.add_trace(alpaca_trace)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        font=dict(family=fonts, size=font_size),\n",
    "        width=w, height=h,\n",
    "        paper_bgcolor='white',\n",
    "        plot_bgcolor='white'\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def add_p_value_annotation(fig, pvals, _format=dict(interline=0.07, text_height=1.07, color='black'),gap_offset = 0.025, row_offset= 0.05, pvals_type = 'numbers',annotation_font_size=12,add_brackets=True):    \n",
    "    y_loc = 1\n",
    "    for box_pair in pvals.keys():\n",
    "        # add horizontal line:\n",
    "        pvalue = pvals[box_pair]\n",
    "        \n",
    "        if pvals_type == 'numbers':\n",
    "            if pvalue < 0.001:\n",
    "                formatted = sigfig.round(pvalue, sigfigs=3, notation='scientific')\n",
    "                base, exponent = formatted.split('E')\n",
    "                symbol = f\"P = {base}Ã—10<sup>{int(exponent)}</sup>\"\n",
    "            else:\n",
    "                symbol = f\"P = {sigfig.round(pvalue, sigfigs=3)}\"                \n",
    "        elif pvals_type == 'stars':\n",
    "            if pvalue >= 0.05:\n",
    "                symbol = 'ns'\n",
    "            elif pvalue >= 0.01: \n",
    "                symbol = '*'\n",
    "            elif pvalue >= 0.001:\n",
    "                symbol = '**'\n",
    "            else:\n",
    "                symbol = '***'\n",
    "        if add_brackets:                \n",
    "            fig.add_shape(type=\"line\",\n",
    "                        xref=\"x\", yref=\"y domain\",\n",
    "                        x0=box_pair[0], y0=y_loc, \n",
    "                        x1=box_pair[1], y1=y_loc,\n",
    "                        line=dict(color=_format['color'], width=2,)\n",
    "                    )\n",
    "            # Vertical line\n",
    "            fig.add_shape(type=\"line\",\n",
    "                xref=\"x\", yref=\"y domain\",\n",
    "                x0=box_pair[0], y0=y_loc, \n",
    "                x1=box_pair[0], y1=y_loc-0.025,\n",
    "                line=dict(color=_format['color'], width=2,)\n",
    "            )\n",
    "            # Vertical line\n",
    "            fig.add_shape(type=\"line\",\n",
    "                xref=\"x\", yref=\"y domain\",\n",
    "                x0=box_pair[1], y0=y_loc, \n",
    "                x1=box_pair[1], y1=y_loc-0.025,\n",
    "                line=dict(color=_format['color'], width=2,)\n",
    "        )\n",
    "\n",
    "        ## add text at the correct x, y coordinates\n",
    "        ## for bars, there is a direct mapping from the bar number to 0, 1, 2...\n",
    "        annotation_x = (box_pair[0] + box_pair[1])/2\n",
    "        annotation_y = y_loc + gap_offset\n",
    "        fig.add_annotation(dict(font=dict(color=_format['color'],size=annotation_font_size),\n",
    "            x=annotation_x,\n",
    "            y=annotation_y,\n",
    "            showarrow=False,\n",
    "            text=symbol,\n",
    "            textangle=0,\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\"\n",
    "        ))\n",
    "        y_loc += row_offset\n",
    "    return fig\n",
    "\n",
    "\n",
    "def wilcoxon_test(list_of_series):\n",
    "    results = {}\n",
    "    for i in range(len(list_of_series)):\n",
    "        for j in range(i + 1, len(list_of_series)):\n",
    "            col1 = list_of_series[i]\n",
    "            col2 = list_of_series[j]\n",
    "            t_stat, p_value = wilcoxon(col1, col2)\n",
    "            results[(i, j)] = p_value\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_H2_alpaca_name_mapping(H2_output_dir):\n",
    "    datasets_original_names = [x for x in os.listdir(H2_output_dir) if 'dataset' in x]\n",
    "    formatted_names = {}\n",
    "    for h2_dataset in datasets_original_names:\n",
    "        file_path = f'{H2_output_dir}/{h2_dataset}/hatchet.ini'\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('bam'):\n",
    "                    bam_line = line\n",
    "                    break\n",
    "        # find experiment name:\n",
    "        bam_line_split = bam_line.split('/')\n",
    "        h2_dataset_without_suffix = '_'.join(h2_dataset.split('_')[:3])\n",
    "        dataset_index = bam_line_split.index(h2_dataset_without_suffix)\n",
    "        experiment_name = bam_line_split[dataset_index+1]\n",
    "        dataset_name_formatted = h2_dataset_without_suffix.replace('_','-')\n",
    "        experiment_name_formatted = experiment_name.replace('_','-')\n",
    "        formatted_names[h2_dataset] = f'{dataset_name_formatted}.{experiment_name_formatted}'\n",
    "    return formatted_names\n",
    "    \n",
    "    \n",
    "def load_H2_results(h2_name,hatchet2_output_dir):\n",
    "    H2_results = pd.read_csv(f'{hatchet2_output_dir}/{h2_name}/results/best.seg.ucn', sep='\\t')\n",
    "    H2_results['#CHR'] = H2_results['#CHR'].apply(lambda x: x.replace('chr',''))\n",
    "    H2_results['START'] = H2_results['START'].astype(int)\n",
    "    H2_results['END'] = H2_results['END'].astype(int)\n",
    "    H2_results.rename(columns={'cn_normal':'cn_clone100'},inplace=True)\n",
    "    position_columns_names=['#CHR','START','END']\n",
    "    H2_results_clones = [x for x in H2_results.columns if x.startswith('cn_')]\n",
    "    H2_results = H2_results[position_columns_names + H2_results_clones].drop_duplicates() \n",
    "    return H2_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = 'mascote'\n",
    "run_name='mascote_default'\n",
    "hatchet_sim_true_cns = f'../../_assets/hatchet-paper/simulation/data'\n",
    "alpaca_output_dir = f'../../output/{cohort}/{run_name}'\n",
    "hatchet2_output_dir = f'../../_assets/hatchet2'\n",
    "hatchet1_sim_input = f'../../_assets/hatchet-paper/simulation/data'\n",
    "H2_alpaca_mapping = get_H2_alpaca_name_mapping(hatchet2_output_dir)\n",
    "colours_path = f'../../_assets/publication_palette.json'\n",
    "colours = get_hd_hatchet_alpaca_colours(colours_path)\n",
    "results = {}\n",
    "results['results_H2'] = {}\n",
    "results['results_ALPACA'] = {}\n",
    "tumour_ids = [x for x in os.listdir(f'{alpaca_output_dir}/patient_outputs') if 'dataset' in x]\n",
    "for tumour_id in tumour_ids:\n",
    "    if tumour_id in list(H2_alpaca_mapping.values()):\n",
    "        dataset_name = tumour_id.split('.')[0].replace('-','_')\n",
    "        experiment = tumour_id.split('.')[1].replace('-','_')\n",
    "        alpaca_results = load_alpaca_hatchet_format(alpaca_output_dir,tumour_id)\n",
    "        true_copynumber_states = load_true_copynumber_states(hatchet1_sim_input,dataset_name)\n",
    "        results['results_ALPACA'][tumour_id] = get_model_metrics(alpaca_results, true_copynumber_states)\n",
    "        reverse_dict = {v: k for k, v in H2_alpaca_mapping.items()}\n",
    "        h2_name = reverse_dict[tumour_id]\n",
    "        h2_name_no_suffix = '_'.join(h2_name.split('_')[:3])\n",
    "        H2_results = load_H2_results(h2_name,hatchet2_output_dir)\n",
    "        results['results_H2'][tumour_id] = get_model_metrics(H2_results, true_copynumber_states)\n",
    "results_df_H2 = pd.DataFrame(results['results_H2']).T\n",
    "results_df_ALPACA = pd.DataFrame(results['results_ALPACA']).T\n",
    "\n",
    "w=200\n",
    "h=600\n",
    "lables_font_size=25\n",
    "annotation_font_size=24\n",
    "metric='accuracy'\n",
    "paired_p_vals = wilcoxon_test([results_df_H2[metric],results_df_ALPACA[metric]])\n",
    "print(metric)\n",
    "print(paired_p_vals)\n",
    "fig = make_plot(results_H1_df=results_df_H2,\n",
    "                results_ALPACA_df=results_df_ALPACA,\n",
    "                colours=colours,\n",
    "                )\n",
    "add_p_value_annotation(fig, paired_p_vals,gap_offset = 0.06, row_offset= 0.09,annotation_font_size=annotation_font_size)\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=130, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(f'../../figures/Suppfig2a_accuracy_comparison_boxplot.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary figure 2: simulated cohort overview\n",
    "pivot_table = pd.read_csv(f'{output_directory}/simulations/simulations_default/cohort_outputs/simulated_cohort_overview.csv',index_col=0)\n",
    "mask = pivot_table == 0\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=False, cbar_kws={'label': 'Tumour count'}, mask=mask)\n",
    "plt.title('Number of clones and samples in simulated cohort')\n",
    "plt.xlabel('Number of clones')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.savefig(f'../../figures/Suppfig2b_simulated_cohort_overview.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary figure 2: single cell clone proportions\n",
    "selected_tumours = ['S0noise0.1']\n",
    "input_data_directory = f'{output_directory}/single_cell/single_cell_default/patient_outputs'\n",
    "cp_table = pd.read_csv(f'{input_data_directory}/cp_table.csv',index_col='clone')\n",
    "cp_table = cp_table.round(3)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "sns.heatmap(cp_table, cmap='Blues', annot=True, cbar_kws={'label': 'Clone proportion'})\n",
    "plt.title('Clone proportions in single cell study')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Clones')\n",
    "plt.savefig(f'../../figures/Suppfig2i_single_cell_clone_proportions.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary Figure 2: simulations example case:\n",
    "import SIMULATIONS_make_example_case_heatmaps\n",
    "tumour_id='LTXSIM127'\n",
    "base_dir = '../../_assets/example_heatmaps'\n",
    "input_data_directory = f'{base_dir}/input/'\n",
    "tumour_input_directory = f'{input_data_directory}/{tumour_id}'\n",
    "true = pd.read_csv(f'{tumour_input_directory}/copynumbers.csv')\n",
    "true = SIMULATIONS_make_example_case_heatmaps.true_unify_format(true)\n",
    "chr_table_file = f'{output_directory}/../_assets/chr_len.csv'\n",
    "\n",
    "wgd_calls = f'{output_directory}/simulations/simulations_default/cohort_outputs/wgd_calls.csv'\n",
    "wgd_calls = pd.read_csv(wgd_calls)\n",
    "tumour_wgd_calls = wgd_calls[wgd_calls.tumour_id == tumour_id]\n",
    "wgd_clones = list(tumour_wgd_calls[tumour_wgd_calls.GD > 0].clones)\n",
    "\n",
    "\n",
    "cohort_results_file = f'{base_dir}/ALPACA/combined.csv'\n",
    "cohort_results = pd.read_csv(cohort_results_file)\n",
    "\n",
    "alpaca_output = cohort_results[cohort_results.tumour_id==tumour_id]\n",
    "alpaca_output = alpaca_output[alpaca_output.clone!='diploid']\n",
    "# use loss_gain heatmap - use up to 8 colours, but use only up to max_cn\n",
    "from python_functions import get_colourmap,create_seaborn_colour_map\n",
    "max_cn_colour_boundry = 8\n",
    "loss_gain_cmap = get_colourmap(max_cn_colour_boundry+1,testing=False,cn_type='allele')\n",
    "sns_cmap = create_seaborn_colour_map(loss_gain_cmap, max_cn_colour_boundry+1)\n",
    "sns_cmap[1] = (1.0, 1.0, 1.0) # ensure 1 is white\n",
    "max_cn = alpaca_output[['pred_CN_A','pred_CN_B']].max().max()\n",
    "# use only first 5 clours:\n",
    "sns_cmap = sns.color_palette(sns_cmap[0:int(max_cn+1)])\n",
    "heatmap_A = SIMULATIONS_make_example_case_heatmaps.plot_heatmap_with_tree_compare_with_true_solution_publication(alpaca_output=alpaca_output,input_data_directory=tumour_input_directory, chr_table_file=chr_table_file,wgd_clones=wgd_clones, max_cpn_cap=8, allele='A', true_solution_df=true, plot_comparison=True, sort_alleles=False,color_map=sns_cmap)\n",
    "heatmap_A.fig.write_image(f'../../figures/Suppfig2c_ALPACA_simulations_{tumour_id}_A_example_heatmap.pdf')\n",
    "heatmap_B = SIMULATIONS_make_example_case_heatmaps.plot_heatmap_with_tree_compare_with_true_solution_publication(alpaca_output=alpaca_output,input_data_directory=tumour_input_directory, chr_table_file=chr_table_file,wgd_clones=wgd_clones, max_cpn_cap=8, allele='B', true_solution_df=true, plot_comparison=True, sort_alleles=False,color_map=sns_cmap)\n",
    "heatmap_B.fig.write_image(f'../../figures/Suppfig2d_ALPACA_simulations_{tumour_id}_B_example_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary Figure 2: simulations simple model:\n",
    "\n",
    "import SIMULATIONS_make_example_case_heatmaps\n",
    "import SIMULATIONS_make_example_case_heatmaps\n",
    "tumour_id='LTXSIM127'\n",
    "base_dir = '../../_assets/example_heatmaps'\n",
    "input_data_directory = f'{base_dir}/input/'\n",
    "tumour_input_directory = f'{input_data_directory}/{tumour_id}'\n",
    "true = pd.read_csv(f'{tumour_input_directory}/copynumbers.csv')\n",
    "true = SIMULATIONS_make_example_case_heatmaps.true_unify_format(true)\n",
    "# remove incorrect chr labels from simulations:\n",
    "#true = true.drop(columns=['CHR','chr'])\n",
    "# create correct value based on the segment name:\n",
    "#true['chr'] = 'chr'+true['segment'].str.split('_').str[0]\n",
    "chr_table_file = f'{output_directory}/../_assets/chr_len.csv'\n",
    "wgd_clones = []\n",
    "\n",
    "cohort_results_file = f'{output_directory}/simulations/simple_model_default/cohort_outputs/combined.csv'\n",
    "cohort_results_file = f'{base_dir}/SM/combined.csv'\n",
    "cohort_results = pd.read_csv(cohort_results_file)\n",
    "\n",
    "\n",
    "alpaca_output = cohort_results[cohort_results.tumour_id==tumour_id]\n",
    "alpaca_output = alpaca_output[alpaca_output.clone!='diploid']\n",
    "# use loss_gain heatmap - use up to 8 colours, but use only up to max_cn\n",
    "from python_functions import get_colourmap,create_seaborn_colour_map\n",
    "max_cn_colour_boundry = 8\n",
    "loss_gain_cmap = get_colourmap(max_cn_colour_boundry+1,testing=False,cn_type='allele')\n",
    "sns_cmap = create_seaborn_colour_map(loss_gain_cmap, max_cn_colour_boundry+1)\n",
    "sns_cmap[1] = (1.0, 1.0, 1.0) # ensure 1 is white\n",
    "alpaca_output[['pred_CN_A','pred_CN_B']] = alpaca_output[['pred_CN_A','pred_CN_B']].astype(int)\n",
    "max_cn = 4\n",
    "# use only first 5 clours:\n",
    "sns_cmap = sns.color_palette(sns_cmap[0:int(max_cn+1)])\n",
    "heatmap_A = SIMULATIONS_make_example_case_heatmaps.plot_heatmap_with_tree_compare_with_true_solution_publication(alpaca_output=alpaca_output,input_data_directory=tumour_input_directory, chr_table_file=chr_table_file,wgd_clones=wgd_clones, max_cpn_cap=8, allele='A', true_solution_df=true, plot_comparison=True, sort_alleles=False,color_map=sns_cmap)\n",
    "heatmap_A.fig.write_image(f'../../figures/Suppfig2e_simple_model_{tumour_id}_A_example_heatmap.pdf')\n",
    "heatmap_B = SIMULATIONS_make_example_case_heatmaps.plot_heatmap_with_tree_compare_with_true_solution_publication(alpaca_output=alpaca_output,input_data_directory=tumour_input_directory, chr_table_file=chr_table_file,wgd_clones=wgd_clones, max_cpn_cap=8, allele='B', true_solution_df=true, plot_comparison=True, sort_alleles=False,color_map=sns_cmap)\n",
    "heatmap_B.fig.write_image(f'../../figures/Suppfig2f_simple_model_{tumour_id}_B_example_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary figure 2: comparison with TUSV-ext\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from functions import compare_copynumber_profiles,get_proportions_whole_tumour\n",
    "from metrics_functions import get_distance_metrics,get_comparison_dfs\n",
    "\n",
    "def convert_clone_proportions(output_path_tumour,vcf_file_names):\n",
    "    clone_proportions = pd.read_csv(f'{output_path_tumour}/U.tsv',sep='\\t',header=None)\n",
    "    clone_proportions.columns = [f'clone{i}' for i in clone_proportions.columns]\n",
    "    clone_proportions = clone_proportions.T\n",
    "    clone_proportions.columns = [x.replace('.vcf','') for x in vcf_file_names]\n",
    "    clone_proportions = clone_proportions.reset_index().rename(columns={'index':'clone'})\n",
    "    return clone_proportions\n",
    "\n",
    "\n",
    "\n",
    "def convert_vcf(vcf_input_path,vcf_file_names):\n",
    "    vcf_input = pd.read_csv(f'{vcf_input_path}/{vcf_file_names[0]}',sep='\\t',header=None,comment='#')\n",
    "    vcf_input = vcf_input [[0,1,2,7]]\n",
    "    vcf_input.columns = ['chromosome', 'start_pos', 'event_id', 'INFO']\n",
    "    vcf_input = vcf_input[vcf_input['event_id'].str.contains('cnv')]\n",
    "    vcf_input['end_pos'] = vcf_input['INFO'].str.split(';').str[0].str.split('=').str[1].astype(int)\n",
    "    vcf_input = vcf_input.drop(columns=['INFO'])\n",
    "    return vcf_input\n",
    "\n",
    "\n",
    "def dot_to_paths(dot_content):\n",
    "    graph = nx.DiGraph(nx.drawing.nx_pydot.read_dot(dot_content))\n",
    "    root = [node for node in graph.nodes if graph.in_degree(node) == 0]\n",
    "    if not root:\n",
    "        raise ValueError(\"No root found in the graph.\")\n",
    "    root = root[0]\n",
    "    def find_paths(node):\n",
    "        successors = list(graph.successors(node))\n",
    "        if not successors:  # Leaf node\n",
    "            return [[node]]\n",
    "        paths = []\n",
    "        for succ in successors:\n",
    "            for path in find_paths(succ):\n",
    "                paths.append([node] + path)\n",
    "        return paths\n",
    "    tree = find_paths(root)\n",
    "    for path in tree:\n",
    "        for i in range(len(path)):\n",
    "            path[i] = \"clone\"+path[i]\n",
    "    return tree\n",
    "\n",
    "\n",
    "def convert_copynumbers(output_path_tumour,vcf_input):\n",
    "    TUSVEXT_copynumbers = pd.read_csv(f'{output_path_tumour}/C.tsv',sep='\\t',header=None)\n",
    "    info_file = pd.read_csv(f'{output_path_tumour}/F_info_phasing.csv',sep='\\t',header=None,names=['chromosome','start_pos','event_id'])\n",
    "    # variables need to retrieve minor and major allele columns:\n",
    "    l_var = info_file['event_id'].str.contains('sv').sum() # number of sampled breakpoints\n",
    "    g_var =  info_file['event_id'].str.contains('snv').sum()# number of sampled SNVs\n",
    "    r_var = info_file[info_file['event_id'].str.contains('cnv')]['event_id'].nunique()# number of sampled CNAs\n",
    "    assert l_var+g_var+(2*r_var) == TUSVEXT_copynumbers.shape[1]\n",
    "    TUSVEXT_copynumbers.columns = info_file['event_id']\n",
    "    TUSVEXT_copynumbers = TUSVEXT_copynumbers.T\n",
    "    TUSVEXT_copynumbers.columns = [f'clone{i}' for i in TUSVEXT_copynumbers.columns]\n",
    "    TUSVEXT_copynumbers['allele'] = ''\n",
    "    TUSVEXT_copynumbers.reset_index(inplace=True)\n",
    "    TUSVEXT_copynumbers.loc[l_var+g_var:l_var+g_var+r_var,'allele'] = 'A'\n",
    "    TUSVEXT_copynumbers.loc[l_var+g_var+r_var:l_var+g_var+2*r_var, 'allele'] = 'B'\n",
    "    TUSVEXT_copynumbers = TUSVEXT_copynumbers[TUSVEXT_copynumbers['allele']!='']\n",
    "    # assign segment ids:\n",
    "    TUSVEXT_copynumbers_long = TUSVEXT_copynumbers.melt(id_vars=['event_id','allele'],value_vars=[i for i in TUSVEXT_copynumbers.columns if 'clone' in i],var_name='clone',value_name='copy_number')\n",
    "    TUSVEXT_copynumbers_af = TUSVEXT_copynumbers_long.pivot(index=['event_id', 'clone'], columns='allele', values='copy_number').reset_index()\n",
    "    TUSVEXT_copynumbers_af.columns = ['event_id', 'clone', 'pred_CN_A', 'pred_CN_B']\n",
    "    TUSVEXT_copynumbers_af = TUSVEXT_copynumbers_af.merge(vcf_input)\n",
    "    # correct start_pos and end_pos so that segment names match with alpaca\n",
    "    TUSVEXT_copynumbers_af['end_pos'] = TUSVEXT_copynumbers_af['end_pos'].astype(int)+1\n",
    "    TUSVEXT_copynumbers_af.loc[TUSVEXT_copynumbers_af['start_pos'] == 1,'start_pos'] = 0\n",
    "    TUSVEXT_copynumbers_af['segment'] = TUSVEXT_copynumbers_af['chromosome'].astype(str)+'_'+TUSVEXT_copynumbers_af['start_pos'].astype(str)+'_'+TUSVEXT_copynumbers_af['end_pos'].astype(str)\n",
    "    TUSVEXT_copynumbers_af.sort_values(['chromosome','start_pos'],inplace=True)\n",
    "    TUSVEXT_copynumbers_af.drop(columns=['event_id'],inplace=True)\n",
    "    \n",
    "    return TUSVEXT_copynumbers_af\n",
    "\n",
    "\n",
    "def convert_tusvext_to_alpaca(tumour_id,publication_dir):\n",
    "    output_path_tumour = f'{publication_dir}/_assets/tusvext/output/{tumour_id}'\n",
    "    vcf_input_path = f'{publication_dir}/_assets/tusvext/input/{tumour_id}'\n",
    "    vcf_file_names = [x for x in os.listdir(vcf_input_path) if 'vcf' in x]\n",
    "    # convert clone proportions\n",
    "    clone_proportions = convert_clone_proportions(output_path_tumour,vcf_file_names)\n",
    "    # get segment identity\n",
    "    vcf_input = convert_vcf(vcf_input_path,vcf_file_names)\n",
    "    # convert tree\n",
    "    tree_path = f'{output_path_tumour}/T.dot'\n",
    "    tree = dot_to_paths(tree_path)\n",
    "    # convert copynumbers\n",
    "    TUSVEXT_copynumbers_af = convert_copynumbers(output_path_tumour,vcf_input)\n",
    "    return clone_proportions,tree,TUSVEXT_copynumbers_af\n",
    "\n",
    "\n",
    "\n",
    "from plotting_functions import make_error_comparison_plot\n",
    "palette_path = f'../../_assets/publication_palette.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort='H2'\n",
    "run_name='default'\n",
    "overwrite = False\n",
    "simulations_input_path = f'../../_assets/tx_analysis'\n",
    "publication_dir = f'../..'\n",
    "alpaca_input_dir = f'../../_assets/benchmarking/H2/alpaca_input'\n",
    "alpaca_output_dir = f'../../_assets/benchmarking/H2/alpaca_output'\n",
    "tusvext_output_dir = f'../../_assets/tusvext'\n",
    "processed_files_path = f'../../_assets/tusvext/data'\n",
    "os.makedirs(processed_files_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tumour_results = []\n",
    "tumour_ids = os.listdir(f'{tusvext_output_dir}/output')\n",
    "for tumour_id in tumour_ids:\n",
    "    output_path_tumour = f'{tusvext_output_dir}/output/{tumour_id}/F.tsv'\n",
    "     # if TUSVext output is missing, skip:\n",
    "    if not os.path.exists(output_path_tumour):\n",
    "        continue\n",
    "    results_dict = {}\n",
    "    processed_output_name = f'{processed_files_path}/{tumour_id}_matching_comparison.csv'\n",
    "    if os.path.exists(processed_output_name) and not overwrite:\n",
    "        results_df = pd.read_csv(processed_output_name)\n",
    "        all_tumour_results.append(results_df)\n",
    "    else:\n",
    "        ALPACA_input = pd.read_csv(f'{alpaca_input_dir}/{tumour_id}/ALPACA_input_table.csv')\n",
    "        ALPACA_output = pd.read_csv(f'{alpaca_output_dir}/patient_outputs/{tumour_id}/final_{tumour_id}.csv').drop_duplicates()\n",
    "        ALPACA_output = ALPACA_output[ALPACA_output.clone!='diploid']\n",
    "        comparator_name = 'TUSVext'\n",
    "        _,_,COMPARATOR_output = convert_tusvext_to_alpaca(tumour_id,publication_dir)\n",
    "        COMPARATOR_output = COMPARATOR_output[COMPARATOR_output['clone']!='diploid']\n",
    "        TRUE_copynumbers = pd.read_csv(f'{alpaca_input_dir}/{tumour_id}/copynumbers.csv')\n",
    "        patient_output_path =  f'{alpaca_output_dir}/patient_outputs/{tumour_id}/'\n",
    "        simulations_path = f'{simulations_input_path}/{tumour_id}/sim'\n",
    "        true_props_path =  f'{simulations_path}/{tumour_id}_cloneprops.tsv'\n",
    "        conipher_props_path = f'{alpaca_input_dir}/{tumour_id}/cp_table.csv'\n",
    "        common_segments = set.intersection(set(COMPARATOR_output['segment']), set(ALPACA_output['segment']), set(TRUE_copynumbers['segment']))\n",
    "        TRUE_copynumbers = TRUE_copynumbers[TRUE_copynumbers['segment'].isin(common_segments)]\n",
    "        ALPACA_output = ALPACA_output[ALPACA_output['segment'].isin(common_segments)]\n",
    "        COMPARATOR_output = COMPARATOR_output[COMPARATOR_output['segment'].isin(common_segments)]\n",
    "        alpaca_score = compare_copynumber_profiles(TRUE_copynumbers,ALPACA_output,metric='hamming')['closest_clone_distance'].mean()\n",
    "        comparator_score = compare_copynumber_profiles(TRUE_copynumbers,COMPARATOR_output,metric='hamming')['closest_clone_distance'].mean()\n",
    "        results_dict[tumour_id] = {'alpaca_score':alpaca_score, f'{comparator_name}':comparator_score}\n",
    "        results_df = pd.DataFrame(results_dict).T.reset_index()\n",
    "        results_df.to_csv(processed_output_name,index=False)\n",
    "        all_tumour_results.append(results_df)\n",
    "matching_distance_results = pd.concat(all_tumour_results)\n",
    "# TVD\n",
    "all_results=[]\n",
    "tumour_ids = matching_distance_results['index']\n",
    "for tumour_id in tumour_ids:\n",
    "    results_tumour_path = f'{processed_files_path}/{tumour_id}_distance_metrics.csv'\n",
    "    if os.path.exists(results_tumour_path):\n",
    "        results_tumour = pd.read_csv(results_tumour_path)\n",
    "        all_results.append(results_tumour)\n",
    "        continue\n",
    "    results_tumour = []\n",
    "    print(tumour_id)\n",
    "    TRUE_copynumbers = pd.read_csv(f'{alpaca_input_dir}/{tumour_id}/copynumbers.csv')\n",
    "    TRUE_proportions = get_proportions_whole_tumour(pd.read_csv(f'{alpaca_input_dir}/{tumour_id}/true_proportions.csv'))\n",
    "    TRUE_proportions['clone'] = 'clone'+TRUE_proportions['clone'].astype(str) if 'clone' not in str(TRUE_proportions.clone.iloc[0]) else TRUE_proportions['clone']\n",
    "    ALPACA_output = pd.read_csv(f'{alpaca_output_dir}/patient_outputs/{tumour_id}/final_{tumour_id}.csv').drop_duplicates()\n",
    "    ALPACA_output = ALPACA_output[ALPACA_output.clone!='diploid']\n",
    "    CONIPHER_proportions = get_proportions_whole_tumour(pd.read_csv(f'{alpaca_input_dir}/{tumour_id}/cp_table.csv'))\n",
    "    COMPARATOR_proportions,_,COMPARATOR_output = convert_tusvext_to_alpaca(tumour_id,publication_dir)\n",
    "    COMPARATOR_output = COMPARATOR_output[COMPARATOR_output['clone']!='diploid']\n",
    "    COMPARATOR_proportions = get_proportions_whole_tumour(COMPARATOR_proportions)\n",
    "    res_df = COMPARATOR_output.merge(COMPARATOR_proportions, on='clone')\n",
    "    common_segments = set.intersection(set(COMPARATOR_output['segment']), set(ALPACA_output['segment']), set(TRUE_copynumbers['segment']))\n",
    "    TRUE_copynumbers = TRUE_copynumbers[TRUE_copynumbers['segment'].isin(common_segments)]\n",
    "    ALPACA_output = ALPACA_output[ALPACA_output['segment'].isin(common_segments)]\n",
    "    COMPARATOR_output = COMPARATOR_output[COMPARATOR_output['segment'].isin(common_segments)]\n",
    "    for seg_ind,segment in enumerate(common_segments):\n",
    "        true_copynumbers_seg = TRUE_copynumbers.query('segment == @segment')\n",
    "        alpaca_segment = ALPACA_output.query('segment == @segment')\n",
    "        comparator_segment = COMPARATOR_output.query('segment == @segment')\n",
    "        #calculate scores for alpaca\n",
    "        comparison_df_A,comparison_df_B = get_comparison_dfs(true_copynumbers_seg,TRUE_proportions,alpaca_segment,CONIPHER_proportions)\n",
    "        output_df_alpaca = get_distance_metrics(comparison_df_A,comparison_df_B,tumour_id,segment)\n",
    "        output_df_alpaca['model'] = 'ALPACA'\n",
    "        #calculate scores for TUSVext\n",
    "        comparison_df_A,comparison_df_B = get_comparison_dfs(true_copynumbers_seg,TRUE_proportions,comparator_segment,COMPARATOR_proportions)\n",
    "        output_df_comparator = get_distance_metrics(comparison_df_A,comparison_df_B,tumour_id,segment)\n",
    "        output_df_comparator['model'] = 'TUSVext'\n",
    "        output_df = pd.concat([output_df_alpaca,output_df_comparator])\n",
    "        results_tumour.append(output_df)\n",
    "    results_tumour = pd.concat(results_tumour)\n",
    "    results_tumour.to_csv(results_tumour_path,index=False)\n",
    "    all_results.append(results_tumour)\n",
    "distance_results = pd.concat(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_distance_results.rename(columns={'alpaca_score':'alpaca','TUSVext':'hatchet'},inplace=True)\n",
    "w=400\n",
    "h=600\n",
    "lables_font_size=25\n",
    "annotation_font_size=12\n",
    "scale=1\n",
    "fig_H, results = make_error_comparison_plot(\n",
    "    cohort_results=matching_distance_results,\n",
    "    palette_path=palette_path,\n",
    "    y_axis_title='Mean Hamming distance<br>to matching true CN profile',\n",
    "    w=w,\n",
    "    h=h,\n",
    "    font_size=lables_font_size,orientation='v')\n",
    "fig_H.update_traces(name=\"TUSV-ext\", selector=dict(name=\"HATCHet\"))\n",
    "fig_H.write_image(f'../../figures/Suppfig2g_hamming_comparison_boxplot_alpaca_vs_tusvext.pdf', width=w, height=h,scale=scale)\n",
    "fig_H.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_results_transformed = distance_results.groupby(['tumour_id','model']).apply(lambda x: x['total_var_dist_mean'].mean()).reset_index().rename(columns={0:'total_var_dist_mean'})\n",
    "distance_results_transformed_wide = distance_results_transformed.pivot(index='tumour_id',columns='model',values='total_var_dist_mean').reset_index()\n",
    "distance_results_transformed_wide.rename(columns={'ALPACA':'alpaca','TUSVext':'hatchet'},inplace=True)\n",
    "w=400\n",
    "h=600\n",
    "lables_font_size=25\n",
    "annotation_font_size=12\n",
    "scale=1\n",
    "fig_T, results = make_error_comparison_plot(\n",
    "    cohort_results=distance_results_transformed_wide,\n",
    "    palette_path=palette_path,\n",
    "    y_axis_title='Mean Total Variation Distance<br>to true CN solution',\n",
    "    w=w,\n",
    "    h=h,\n",
    "    font_size=lables_font_size,orientation='v')\n",
    "fig_T.update_traces(name=\"TUSV-ext\", selector=dict(name=\"HATCHet\"))\n",
    "fig_T.write_image(f'../../figures/Suppfig2h_TVD_comparison_boxplot_alpaca_vs_tusvext.pdf', width=w, height=h,scale=scale)\n",
    "fig_T.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
